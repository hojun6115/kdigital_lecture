{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 웹크롤링 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    전통적인 요리법이나 양식은 상당한 차이가 있지만, 이탈리아 요리는 다른 국가의 요리 문화에서 다양한 영감을 줄 만큼 다양하고 혁신적인 것으로 평가되고 있다. 각 지방마다 고유의 특색이 있어 그 양식도 다양하지만 크게 북부와 남부로 나눌 수 있다. 다른 나라와 국경을 맞대고 있던 북부 지방은 산업화되어 경제적으로 풍족하고 농업이 발달해 쌀이 풍부해 유제품이 다양한 반면 경제적으로 침체되었던 남부 지방은 올리브와 토마토, 모차렐라 치즈가 유명하고 특별히 해산물을 활용한 요리가 많다. 식재료와 치즈 등의 차이는 파스타의 종류와 소스와 수프 등도 다름을 의미한다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#실습01\n",
    "url = \"http://scrapying-study.firebaseapp.com/01\"\n",
    "reponse = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(reponse.text, \"html.parser\")\n",
    "\n",
    "cook = soup.find(id='cook')\n",
    "print(cook.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'이름': '이몽룡', '나이': '34'}, {'이름': '홍길동', '나이': '23'}]\n"
     ]
    }
   ],
   "source": [
    "#실습02\n",
    "url = \"http://scrapying-study.firebaseapp.com/01\"\n",
    "reponse = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(reponse.text, \"html.parser\")\n",
    "\n",
    "results = soup.find_all('th','tablehead')\n",
    "results2 = []\n",
    "for i in range(2):\n",
    "    results2.append(results[i].text)\n",
    "\n",
    "values = soup.find_all('td')\n",
    "values_mong = []\n",
    "values_hong = []\n",
    "for j in range(2):\n",
    "    values_mong.append(values[j].text)\n",
    "    values_hong.append(values[j+2].text)\n",
    "    \n",
    "dic1 = dict(zip(results2,values_mong))\n",
    "dic2 = dict(zip(results2,values_hong))\n",
    "\n",
    "dictionary_result = [dic1, dic2]\n",
    "print(dictionary_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    크롤링 연습사이트 01-1 페이지입니다.\n",
      "\n",
      "\n",
      "    크롤링 연습사이트 01-2 페이지입니다.\n",
      "\n",
      "\n",
      "    크롤링 연습사이트 01-3 페이지입니다.\n",
      "\n",
      "\n",
      "    크롤링 연습사이트 01-4 페이지입니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#실습03\n",
    "url = \"https://scrapying-study.firebaseapp.com/01/\"\n",
    "reponse = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(reponse.text, \"html.parser\")\n",
    "atag = soup.find_all('a')\n",
    "\n",
    "for i in range(4):\n",
    "    new_atag = \"0{}.html\".format(i+1)\n",
    "    new_url = \"https://scrapying-study.firebaseapp.com/01/{}\".format(new_atag)\n",
    "    new_response = requests.get(new_url)\n",
    "    new_soup = BeautifulSoup(new_response.text,\"html.parser\")\n",
    "    result = new_soup.find('p')\n",
    "    print(result.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실습 강사님 모범답안"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'이름': '이몽룡', '나이': '34'}, {'이름': '홍길동', '나이': '23'}]\n"
     ]
    }
   ],
   "source": [
    "#실습01 쉬우니까 패스\n",
    "#실습02 : 부모, 자식 객체 활용!! 훨씬 간단하게 코딩 가능\n",
    "\n",
    "result = []\n",
    "headers = soup.find_all('th')\n",
    "keys = []\n",
    "for th in headers:\n",
    "    keys.append(th.text)\n",
    "\n",
    "tbody = soup.find(\"tbody\")\n",
    "tr_list = tbody.find_all(\"tr\")\n",
    "for tr in tr_list:\n",
    "    td_list = tr.find_all(\"td\")\n",
    "    values = []\n",
    "    for td in td_list:\n",
    "        values.append(td.text)\n",
    "    result.append(dict(zip(keys,values)))\n",
    "    \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    크롤링 연습사이트 01-1 페이지입니다.\n",
      "\n",
      "\n",
      "    크롤링 연습사이트 01-2 페이지입니다.\n",
      "\n",
      "\n",
      "    크롤링 연습사이트 01-3 페이지입니다.\n",
      "\n",
      "\n",
      "    크롤링 연습사이트 01-4 페이지입니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#실습03 : href를 가져오는 것이 핵심(attrs 활용!)\n",
    "\n",
    "#앞부분 이미 적용됐으니 생략\n",
    "a_list = soup.find_all('a')\n",
    "\n",
    "for a in a_list:\n",
    "    a.attrs['href']  #href에 대한 키값을 가져온다!\n",
    "    new_url = url + \"/\"+ a.attrs['href']\n",
    "    res = requests.get(new_url)\n",
    "    s = BeautifulSoup(res.text,\"html.parser\")\n",
    "    print(s.find('p').text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<div class=\"bold blue\" id=\"title\">\n",
      "        안녕하세요 \n",
      "    </div>]\n"
     ]
    }
   ],
   "source": [
    "URL = \"https://scrapying-study.firebaseapp.com/02/\"\n",
    "response = requests.get(URL)\n",
    "soup = BeautifulSoup(response.text,\"html.parser\")\n",
    "result = soup.select(\"#title\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<div class=\"bold blue\" id=\"title\">\n",
      "        안녕하세요 \n",
      "    </div>, <div class=\"bold\" id=\"content\">\n",
      "<ul>\n",
      "<li>첫번쨰리스트</li>\n",
      "<li class=\"blue\">두번째리스트</li>\n",
      "<li class=\"blue\">세번째리스트</li>\n",
      "<li>네번째리스트</li>\n",
      "</ul>\n",
      "</div>]\n"
     ]
    }
   ],
   "source": [
    "URL = \"https://scrapying-study.firebaseapp.com/02/\"\n",
    "response = requests.get(URL)\n",
    "soup = BeautifulSoup(response.text,\"html.parser\")\n",
    "result = soup.select(\".bold\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a href=\"http://google.com\" target=\"_self\">구글</a>]\n"
     ]
    }
   ],
   "source": [
    "URL = \"https://scrapying-study.firebaseapp.com/02/\"\n",
    "response = requests.get(URL)\n",
    "soup = BeautifulSoup(response.text,\"html.parser\")\n",
    "result = soup.select('a[href*=\"google\"]') #a라는 태그 다 가져옴\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<p>온세상이 떨릴듯</p>, <p class=\"blue\">두근거리고 <span>익숙한 듯 편안해</span></p>, <p>네가 느껴져</p>, <p class=\"blue\">오래된 친구같아</p>]\n"
     ]
    }
   ],
   "source": [
    "URL = \"https://scrapying-study.firebaseapp.com/02/\"\n",
    "response = requests.get(URL)\n",
    "soup = BeautifulSoup(response.text,\"html.parser\")\n",
    "result = soup.select('div#winter p')  #후손셀렉터\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<p>온세상이 떨릴듯</p>, <p class=\"blue\">두근거리고 <span>익숙한 듯 편안해</span></p>]\n"
     ]
    }
   ],
   "source": [
    "URL = \"https://scrapying-study.firebaseapp.com/02/\"\n",
    "response = requests.get(URL)\n",
    "soup = BeautifulSoup(response.text,\"html.parser\")\n",
    "result = soup.select('div#winter > p')  #자식셀렉터\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"blue\">두근거리고 <span>익숙한 듯 편안해</span></p>,\n",
       " <p class=\"blue\">오래된 친구같아</p>]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URL = \"https://scrapying-study.firebaseapp.com/02/\"\n",
    "response = requests.get(URL)\n",
    "soup = BeautifulSoup(response.text,\"html.parser\")\n",
    "\n",
    "result = soup.select('p.blue')  #p태그 중 blue인것\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSS 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['써니전자', '5,000'], ['삼성전자', '55,200'], ['안랩', '81,000'], ['케이엠더블..', '57,300'], ['피피아이', '12,600'], ['KT&G', '92,500'], ['삼성전자우', '45,600'], ['대양금속', '10,550'], ['SK하이닉스', '94,700'], ['SK텔레콤', '234,000']]\n",
      "[['다우산업', '28,647.43'], ['나스닥', '9,015.03'], ['홍콩H', '11,320.56'], ['상해종합', '3,085.20'], ['니케이225', '23,656.62']]\n"
     ]
    }
   ],
   "source": [
    "#실습01\n",
    "\n",
    "url = \"https://scrapying-study.firebaseapp.com/03/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text,\"html.parser\")\n",
    "up1_firm = soup.select('a[href*=\"item\"]')\n",
    "up1_price = soup.select('span')\n",
    "\n",
    "up1_list = []\n",
    "\n",
    "for i in range(len(up1_firm)):\n",
    "    up1_list.append([up1_firm[i].text,up1_price[i].text])\n",
    "print(up1_list)\n",
    "\n",
    "up1_foreign = soup.select('a[href*=\"world\"]')\n",
    "up1_foreignlist = []\n",
    "\n",
    "for j in range(len(up1_foreign)):\n",
    "    up1_foreignlist.append([up1_foreign[j].text,up1_price[j+len(up1_firm)].text])\n",
    "\n",
    "    \n",
    "print(up1_foreignlist)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['써니전자', '상한'], ['삼성전자', '하한'], ['안랩', '상한'], ['케이엠더블..', '상한'], ['피피아이', '상한'], ['KT&G', '하한'], ['삼성전자우', '상한'], ['대양금속', '하한'], ['SK하이닉스', '상한'], ['SK텔레콤', '하한']]\n",
      "[['다우산업', '상한'], ['나스닥', '상한'], ['홍콩H', '상한'], ['상해종합', '상한'], ['니케이225', '하한']]\n"
     ]
    }
   ],
   "source": [
    "#실습02\n",
    "\n",
    "up2_firm = soup.select('a[href*=\"item\"]')\n",
    "up2_price = soup.select('img')\n",
    "up2_list = []\n",
    "\n",
    "for i in range(len(up2_firm)):\n",
    "    if up2_price[i]['alt'] == '상승':\n",
    "        up2_price[i]['alt'] = '상한'\n",
    "    elif up2_price[i]['alt'] == '하락':\n",
    "        up2_price[i]['alt'] = '하한'\n",
    "    up2_list.append([up2_firm[i].text, up2_price[i]['alt']])\n",
    "\n",
    "print(up2_list)\n",
    "\n",
    "up2_foreign = soup.select('a[href*=\"world\"]')\n",
    "up2_foreignlist = []\n",
    "\n",
    "for j in range(len(up2_foreign)):\n",
    "    if up2_price[j+len(up2_firm)]['alt'] == '상승':\n",
    "        up2_price[j+len(up2_firm)]['alt'] = '상한'\n",
    "    elif up2_price[j+len(up2_firm)]['alt'] == '하락':\n",
    "        up2_price[j+len(up2_firm)]['alt'] = '하한'\n",
    "    up2_foreignlist.append([up2_foreign[j].text, up2_price[j+len(up2_firm)]['alt']])\n",
    "    \n",
    "print(up2_foreignlist)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['써니전자', '5,000'], ['안랩', '81,000'], ['케이엠더블..', '57,300'], ['피피아이', '12,600'], ['삼성전자우', '45,600'], ['SK하이닉스', '94,700']]\n",
      "[['다우산업', '28,647.43'], ['나스닥', '9,015.03'], ['홍콩H', '11,320.56'], ['상해종합', '3,085.20']]\n"
     ]
    }
   ],
   "source": [
    "#실습03\n",
    "\n",
    "up3_firm = soup.select('a[href*=\"item\"]')\n",
    "up3_value = soup.select('img')\n",
    "up3_price = soup.select('span')\n",
    "up3_list = []\n",
    "\n",
    "for i in range(len(up3_firm)):\n",
    "    if up3_value[i]['alt'] == '상한' or up3_value[i]['alt'] == '상승':\n",
    "        up3_list.append([up3_firm[i].text, up3_price[i].text])\n",
    "                \n",
    "print(up3_list)\n",
    "\n",
    "up3_foreign = soup.select('a[href*=\"world\"]')\n",
    "up3_foreignlist = []\n",
    "\n",
    "for j in range(len(up3_foreign)):\n",
    "    if up3_value[j+len(up3_firm)]['alt'] == '상한' or up3_value[j+len(up3_firm)]['alt'] == '상승':\n",
    "        up3_foreignlist.append([up3_foreign[j].text, up3_price[j+len(up3_firm)].text])\n",
    "\n",
    "print(up3_foreignlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'이름': 'H하우스장위', '분양가': '16,000', '유형': '아파트', '분양유형': '일반민간임대', '세대수': '분양 134세대', '평형': '45㎡~65㎡'}, {'이름': '고덕리엔파크2단지 장기전세', '분양가': '38,400', '유형': '아파트', '분양유형': '장기전세주택', '세대수': '분양 1세대', '평형': '149㎡'}, {'이름': '신정이펜하우스3단지 장기전세', '분양가': '39,040', '유형': '아파트', '분양유형': '장기전세주택', '세대수': '분양 1세대', '평형': '148㎡'}, {'이름': '천왕이펜하우스2단지 장기전세', '분양가': '38,240', '유형': '아파트', '분양유형': '장기전세주택', '세대수': '분양 1세대', '평형': '142㎡'}, {'이름': '송파파크데일2단지 장기전세', '분양가': '45,600', '유형': '아파트', '분양유형': '장기전세주택', '세대수': '분양 1세대', '평형': '150㎡'}]\n"
     ]
    }
   ],
   "source": [
    "#실습04 : 자식or후손 클래스 / split 활용\n",
    "\n",
    "key = ['이름','분양가','유형','분양유형','세대수','평형']\n",
    "result_04 = []\n",
    "\n",
    "names = soup.select('a[target=\"_blank\"]')\n",
    "name_list = []\n",
    "\n",
    "prices = soup.select('dl strong')\n",
    "price_list = []\n",
    "\n",
    "types = soup.select('dl dd')\n",
    "type1_list = []  #아파트 들어갈 것\n",
    "type2_list = []  #일반민간임대 들어갈 것\n",
    "\n",
    "size1_list = []\n",
    "size2_list = []\n",
    "\n",
    "for i in range(5):\n",
    "    name_list.append(names[i].text)\n",
    "    price_list.append(prices[i].text)\n",
    "    type1_list.append(types[4*i+1].text.split('|')[0])\n",
    "    type2_list.append(types[4*i+1].text.split('|')[1])\n",
    "    size1_list.append(types[4*i+2].text.split('|')[0])\n",
    "    size2_list.append(types[4*i+2].text.split('|')[1])\n",
    "\n",
    "house = []    \n",
    "    \n",
    "for j in range(5):\n",
    "    house.append([name_list[j],price_list[j],type1_list[j],type2_list[j],size1_list[j],size2_list[j]])\n",
    "    result_04.append(dict(zip(key,house[j])))\n",
    "\n",
    "\n",
    "print(result_04)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실습 강사님 모범답안  -> 다음날 리뷰한다고 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['써니전자', '5,000'], ['삼성전자', '55,200'], ['안랩', '81,000'], ['케이엠더블..', '57,300'], ['피피아이', '12,600'], ['KT&G', '92,500'], ['삼성전자우', '45,600'], ['대양금속', '10,550'], ['SK하이닉스', '94,700'], ['SK텔레콤', '234,000']]\n"
     ]
    }
   ],
   "source": [
    "#실습01\n",
    "\n",
    "url = \"https://scrapying-study.firebaseapp.com/03/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text,\"html.parser\")\n",
    "li_list = soup.select(\"#popularItemList > li\")\n",
    "result = []\n",
    "\n",
    "for li in li_list:\n",
    "    result.append([li.select_one('a').text, li.select_one('span').text])\n",
    "    \n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하락\n"
     ]
    }
   ],
   "source": [
    "#실습02 : 상승을 상한으로 바꾸고, 하락을 하한으로 바꾸기 추가해야\n",
    "#색깔이 다르다는 것 활용할 수 있음. CSS / class가 up, dn으로 분류되어 있음\n",
    "\n",
    "print(li.select_one('img').attrs['alt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "#실습03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'아파트'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#실습04\n",
    "\n",
    "types[1].text.split('|')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정규표현식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['에스프레소', ' 아메리카노', ' 카페라테', ' 카푸치노']"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coffee = \"에스프레소, 아메리카노, 카페라테, 카푸치노\"\n",
    "coffee.split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "text = \"I like orange! I love orange!\"\n",
    "result = re.match(\"orange\",text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(7, 13), match='orange'>\n"
     ]
    }
   ],
   "source": [
    "text = \"I like orange! I love orange!\"\n",
    "result = re.search(\"orange\",text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'orange'"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 13)"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.span()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(7, 13), match='orange'>\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<callable_iterator object at 0x10aeba4e0>\n"
     ]
    }
   ],
   "source": [
    "result = re.finditer(\"orange\",text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['에스프레소', '아메리카노', '카페라테', '카푸치노']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"에스프레소  \\n\\n  아메리카노  \\n 카페라테 카푸치노 \\n\\n\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "study"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
